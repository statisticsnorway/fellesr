---
title: "DAPLA jukseark"
author: "Sindre Mikael Haugen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DAPLA jukseark}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

### Wrapper funksjoner fra arrow

+ `read_parquet`: Lese parquet
+ `open_dataset`
+ `write_parquet`: Skrive parquet

+ `read_feather` (mangler eksempel + test)
+ `write_feather` (mangler eksempel + test)

+ `read_csv`
+ `write_csv`

+ `read_json`

+ `list.files`

### Mangler 
+ `saveRDS`
+ `readRDS`

## Lese inn data

### Lese inn .parquet-fil
Funksjonen `read_parquet` kan brukes til å lese inn .parquet-filer fra GCS ved å oppgi sti til "bøtten" og filnavn (fungerer med å uten endelsen ".parquet"). Det er også mulig å kun lese inn valgte kolonner og kolonnenavnene spesifiseres i `col_select`.

```{r, eval=FALSE}
data <- read_parquet(bucket = "ssb-prod-dapla-felles-data-delt/R_smoke_test", file = "1987")

data <- read_parquet(bucket = "ssb-prod-dapla-felles-data-delt/R_smoke_test", file = "1987", 
                     col_select = c("Year", "Month"))
```

### Lese inn deler av datasett (.parquet-filer m.m.)

Funksjonen `open_dataset` kan brukes til å lese deler av datasett (bl.a. .parquet-, .feather- og .csv-filer) fra GCS. Det lages en forbindelse til mappen der filen ligger og deretter kan man bruke argumenter fra `dplyr`, som f.eks. `filter` og `select`, før man bruker `collect` til å lese inn dataene i R. For øyeblikket er det ikke mulig å kun lese inn én fil som ligger i en mappe så dersom dataene ikke er "partitioned" må filen ligge i en egen undermappe dersom den skal lese inn alene.

```{r, eval=FALSE}
data <- open_dataset(bucket = "ssb-prod-spesh-personell-data-kilde/2020") %>%
 dplyr::filter(FRTK_ID_SSB == "964977402") %>%
 dplyr::select(ARB_ARBMARK_STATUS, PERS_SUM_ARBEIDSTID, FRTK_ID_SSB, FRTK_ID_SSB) %>%
 dplyr::collect()
```

Funksjonen `open_dataset` kan også brukes til å lese inn sf-objekter (lagret som .parquet-fil med pakken `sfarrow`).

```{r, eval=FALSE}
 data <- open_dataset(bucket = "ssb-prod-spesh-personell-data-kilde/Vegnett") %>%
 dplyr::filter(municipality == "301") %>%
 sfarrow::read_sf_dataset()
```

## Skrive data til Google Cloud Storage

### Skrive .parquet filer til GCS

Funksjonen `write_parquet` kan brukes til å skrive .parquet-filer til GCS bucket.

```{r, eval=FALSE}
write_parquet(data, bucket = "ssb-prod-dapla-felles-data-delt/R_smoke_test", file = "tester_write_parquet.parquet")
```



