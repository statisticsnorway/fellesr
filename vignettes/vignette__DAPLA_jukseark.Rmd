---
title: "DAPLA jukseark"
author: "Sindre Mikael Haugen"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DAPLA jukseark}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Her finner du en oversikt over hvordan å lese inn data fra Google Cloud Storage (DAPLA) til R. 

Siden er under arbeid og pakken er enda ikke installert i Jupyterlab. I mellomtiden kan funksjonene brukes ved å source R-scriptet som ligger her: https://github.com/statisticsnorway/fellesr/blob/main/R/DAPLA_funcs.R

### Wrapper funksjoner fra arrow

#### Funksjonen `read_SSB` inneholder følgende wrapper-funksjoner fra arrow:
+ `read_parquet`:
+ `open_dataset`
+ `read_feather` 
+ `read_csv`
+ `read_json`

#### Funksjonen `write_SSB` inneholder følgende wrapper-funksjoner fra arrow:
+ `write_parquet`
+ `write_feather`
+ `write_csv`

#### Andre funksjoner
+ `list.files`
+ `gcs_bucket`


### Mangler 
+ `write_dataset`
+ `saveRDS`
+ `readRDS`


## Lese inn data fra Google Cloud Storage

Funksjonen `read_SSB` kan brukes til å lese inn .parquet-, .feather-, .csv- og json-filer fra Google Cloud Storage ved å oppgi full sti til "bøtten" + filnavn. Det er også mulig å kun lese inn valgte kolonner og kolonnenavnene spesifiseres i `col_select`. Dersom filen er lagret som en "multifile dataset" (partitioned?), dvs. en fil som ser ut som en mappe, kan denne lese inn ved å skrive navnet på mappen (uten endelsen .parquet). 

```{r, eval=FALSE}
# Parquet
read_SSB_parquet <- read_SSB("ssb-prod-dapla-felles-data-delt/R_smoke_test/1987.parquet")
read_SSB_parquet <- read_SSB("ssb-prod-dapla-felles-data-delt/R_smoke_test/1987.parquet", col_select = c("Year", "Month"))
read_SSB_mfd <- read_SSB("ssb-prod-spesh-personell-data-kilde/enda_en_ny_mappe")

# Feather
read_SSB_feather <- read_SSB("ssb-prod-dapla-felles-data-delt/R_smoke_test/1987.feather")

# CSV
read_SSB_csv <- read_SSB("ssb-prod-dapla-felles-data-delt/R_smoke_test/1987.csv")

# JSON
read_SSB_json <- read_SSB("ssb-prod-spesh-personell-data-kilde/example_1.json") # OBS: flytt denne til ssb-prod-dapla-felles-data-delt
```


### Lese inn deler av datasett (.parquet-filer m.m.)

Funksjonen `open_dataset` kan brukes til å lese deler av datasett (bl.a. .parquet-, .feather- og .csv-filer) fra Google Cloud Storage. Det lages en forbindelse til mappen der filen ligger og deretter kan man bruke argumenter fra `dplyr`, som f.eks. `filter` og `select`, før man bruker `collect` til å lese inn dataene i R. For øyeblikket er det ikke mulig å kun lese inn én fil som ligger i en mappe så dersom dataene ikke er "partitioned" må filen ligge i en egen undermappe dersom den skal lese inn alene.

```{r, eval=FALSE}
data <- open_dataset("ssb-prod-dapla-felles-data-delt/R_smoke_test/1987_1996_dataset") %>%
 dplyr::filter(Year == 1996 & TailNum == "N2823W") %>%
 dplyr::select(Year, Month, DayofMonth, TailNum) %>%
 dplyr::collect()
```

### Lese inn sf-objekt lagret som .parquet-fil

Funksjonen `open_dataset` kan også brukes til å lese inn sf-objekter (lagret som .parquet-fil med pakken `sfarrow`).

```{r, eval=FALSE}
 data <- open_dataset(bucket = "ssb-prod-spesh-personell-data-kilde/Vegnett") %>%
 dplyr::filter(municipality == "301") %>%
 sfarrow::read_sf_dataset()
```


## Skrive data til Google Cloud Storage

Funksjonen `write_SSB` kan brukes til å skrive .parquet-, .feather- og .csv-filer til Google Cloud Storage.

```{r, eval=FALSE}
# PARQUET
write_SSB(data, "ssb-prod-dapla-felles-data-delt/R_smoke_test/write_SSB_parquet_test.parquet")

# FEATHER
write_SSB(data, "ssb-prod-dapla-felles-data-delt/R_smoke_test/write_SSB_parquet_test.feather")

# CSV
write_SSB(data, "ssb-prod-dapla-felles-data-delt/R_smoke_test/write_SSB_parquet_test.csv")
```



